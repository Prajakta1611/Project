{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d352946",
   "metadata": {},
   "source": [
    "# Importing Modeules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f871d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a4f65",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eeadb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"D:\\python\\LstmProject\\india_us.txt\",'r',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db09a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272da452",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyToken = Tokenizer()\n",
    "MyToken.fit_on_texts([file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e02e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word = len(MyToken.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d48ad9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'in': 3,\n",
       " 'a': 4,\n",
       " 'and': 5,\n",
       " 'india': 6,\n",
       " 'of': 7,\n",
       " 'mr': 8,\n",
       " 'us': 9,\n",
       " 'is': 10,\n",
       " 'modi': 11,\n",
       " 'has': 12,\n",
       " 'that': 13,\n",
       " 'biden': 14,\n",
       " 'as': 15,\n",
       " 'with': 16,\n",
       " 'not': 17,\n",
       " \"india's\": 18,\n",
       " 'for': 19,\n",
       " 'but': 20,\n",
       " 'on': 21,\n",
       " 'washington': 22,\n",
       " 'an': 23,\n",
       " 'it': 24,\n",
       " 'says': 25,\n",
       " 'will': 26,\n",
       " 'are': 27,\n",
       " 'indian': 28,\n",
       " 'prime': 29,\n",
       " 'minister': 30,\n",
       " 'this': 31,\n",
       " 'have': 32,\n",
       " 'trade': 33,\n",
       " 'visit': 34,\n",
       " 'by': 35,\n",
       " 'president': 36,\n",
       " 'relationship': 37,\n",
       " 'been': 38,\n",
       " 'about': 39,\n",
       " 'strategic': 40,\n",
       " 'more': 41,\n",
       " 'up': 42,\n",
       " 'during': 43,\n",
       " 'also': 44,\n",
       " 'make': 45,\n",
       " 'sirohi': 46,\n",
       " 'china': 47,\n",
       " 'air': 48,\n",
       " 'force': 49,\n",
       " 'state': 50,\n",
       " 'his': 51,\n",
       " 'ties': 52,\n",
       " 'two': 53,\n",
       " 'at': 54,\n",
       " 'white': 55,\n",
       " 'house': 56,\n",
       " 'be': 57,\n",
       " 'lot': 58,\n",
       " 'ms': 59,\n",
       " 'technology': 60,\n",
       " 'jet': 61,\n",
       " 'semiconductor': 62,\n",
       " 'now': 63,\n",
       " 'first': 64,\n",
       " 'was': 65,\n",
       " 'global': 66,\n",
       " 'one': 67,\n",
       " 'he': 68,\n",
       " 'without': 69,\n",
       " 'they': 70,\n",
       " 'narendra': 71,\n",
       " 'joe': 72,\n",
       " 'world': 73,\n",
       " 'potential': 74,\n",
       " 'how': 75,\n",
       " 'become': 76,\n",
       " 'kugelman': 77,\n",
       " 'years': 78,\n",
       " 'there': 79,\n",
       " \"modi's\": 80,\n",
       " 'fighter': 81,\n",
       " 'from': 82,\n",
       " 'which': 83,\n",
       " 'russia': 84,\n",
       " 'sides': 85,\n",
       " '2023': 86,\n",
       " 'seen': 87,\n",
       " 'under': 88,\n",
       " 'administration': 89,\n",
       " 'or': 90,\n",
       " 'way': 91,\n",
       " 'policy': 92,\n",
       " 'partner': 93,\n",
       " 'autonomy': 94,\n",
       " \"didn't\": 95,\n",
       " 'could': 96,\n",
       " 'joint': 97,\n",
       " 'major': 98,\n",
       " 'following': 99,\n",
       " \"country's\": 100,\n",
       " 'partnership': 101,\n",
       " 'most': 102,\n",
       " 'strengthening': 103,\n",
       " 'between': 104,\n",
       " 'nations': 105,\n",
       " 'closer': 106,\n",
       " 'than': 107,\n",
       " 'time': 108,\n",
       " 'said': 109,\n",
       " 'may': 110,\n",
       " 'broad': 111,\n",
       " 'reason': 112,\n",
       " 'can': 113,\n",
       " \"china's\": 114,\n",
       " 'indo': 115,\n",
       " 'pacific': 116,\n",
       " 'had': 117,\n",
       " 'because': 118,\n",
       " 'later': 119,\n",
       " 'purchase': 120,\n",
       " 'former': 121,\n",
       " 'general': 122,\n",
       " 'owned': 123,\n",
       " 'only': 124,\n",
       " 'wants': 125,\n",
       " 'arms': 126,\n",
       " 'sharing': 127,\n",
       " 'military': 128,\n",
       " 'drones': 129,\n",
       " 'facility': 130,\n",
       " 'into': 131,\n",
       " 'biggest': 132,\n",
       " \"washington's\": 133,\n",
       " 'invest': 134,\n",
       " 'test': 135,\n",
       " 'maker': 136,\n",
       " 'research': 137,\n",
       " 'all': 138,\n",
       " 'future': 139,\n",
       " 'u': 140,\n",
       " 's': 141,\n",
       " 'welcome': 142,\n",
       " 'june': 143,\n",
       " 'dc': 144,\n",
       " 'many': 145,\n",
       " 'never': 146,\n",
       " 'place': 147,\n",
       " 'foreign': 148,\n",
       " 'wanted': 149,\n",
       " 'what': 150,\n",
       " 'some': 151,\n",
       " 'relations': 152,\n",
       " 'close': 153,\n",
       " 'obama': 154,\n",
       " 'trump': 155,\n",
       " 'further': 156,\n",
       " 'did': 157,\n",
       " 'importance': 158,\n",
       " 'free': 159,\n",
       " 'united': 160,\n",
       " 'states': 161,\n",
       " 'usaf': 162,\n",
       " 'personnel': 163,\n",
       " 'front': 164,\n",
       " 'april': 165,\n",
       " 'full': 166,\n",
       " 'over': 167,\n",
       " 'tariffs': 168,\n",
       " 'were': 169,\n",
       " 'discussions': 170,\n",
       " 'disputes': 171,\n",
       " \"it's\": 172,\n",
       " 'democratic': 173,\n",
       " 'hindu': 174,\n",
       " 'party': 175,\n",
       " 'mutual': 176,\n",
       " '\\ufeff': 177,\n",
       " 'lavish': 178,\n",
       " 'called': 179,\n",
       " 'among': 180,\n",
       " 'consequential': 181,\n",
       " \"bbc's\": 182,\n",
       " 'vikas': 183,\n",
       " 'pandey': 184,\n",
       " 'soutik': 185,\n",
       " 'biswas': 186,\n",
       " 'explore': 187,\n",
       " 'factors': 188,\n",
       " 'contribute': 189,\n",
       " \"visit's\": 190,\n",
       " \"us's\": 191,\n",
       " \"world's\": 192,\n",
       " 'populous': 193,\n",
       " 'country': 194,\n",
       " 'stronger': 195,\n",
       " 'dynamic': 196,\n",
       " 'any': 197,\n",
       " 'history': 198,\n",
       " 'completion': 199,\n",
       " 'pomp': 200,\n",
       " 'filled': 201,\n",
       " 'remark': 202,\n",
       " 'exaggeration': 203,\n",
       " 'summit': 204,\n",
       " 'suggests': 205,\n",
       " 'transformed': 206,\n",
       " 'underscores': 207,\n",
       " 'just': 208,\n",
       " 'deep': 209,\n",
       " 'relatively': 210,\n",
       " 'short': 211,\n",
       " 'michael': 212,\n",
       " 'wilson': 213,\n",
       " 'center': 214,\n",
       " 'american': 215,\n",
       " 'think': 216,\n",
       " 'tank': 217,\n",
       " 'key': 218,\n",
       " 'keen': 219,\n",
       " 'draw': 220,\n",
       " 'so': 221,\n",
       " 'act': 222,\n",
       " 'counterbalance': 223,\n",
       " 'growing': 224,\n",
       " 'influence': 225,\n",
       " 'lived': 226,\n",
       " 'their': 227,\n",
       " 'promise': 228,\n",
       " 'landmark': 229,\n",
       " 'civilian': 230,\n",
       " 'nuclear': 231,\n",
       " 'deal': 232,\n",
       " '2005': 233,\n",
       " 'liability': 234,\n",
       " 'law': 235,\n",
       " 'passed': 236,\n",
       " 'three': 237,\n",
       " 'hobbled': 238,\n",
       " 'reactors': 239,\n",
       " 'followed': 240,\n",
       " 'fading': 241,\n",
       " 'commitment': 242,\n",
       " 'manmohan': 243,\n",
       " \"singh's\": 244,\n",
       " 'second': 245,\n",
       " 'term': 246,\n",
       " 'leader': 247,\n",
       " 'coalition': 248,\n",
       " 'government': 249,\n",
       " 'enthusiasm': 250,\n",
       " 'embracing': 251,\n",
       " 'given': 252,\n",
       " 'overall': 253,\n",
       " 'directive': 254,\n",
       " 'work': 255,\n",
       " 'seema': 256,\n",
       " 'author': 257,\n",
       " 'friends': 258,\n",
       " 'benefits': 259,\n",
       " 'story': 260,\n",
       " 'put': 261,\n",
       " 'effort': 262,\n",
       " 'substantive': 263,\n",
       " 'deliverables': 264,\n",
       " 'defence': 265,\n",
       " 'industrial': 266,\n",
       " 'cooperation': 267,\n",
       " 'topped': 268,\n",
       " 'list': 269,\n",
       " 'consider': 270,\n",
       " 'electric': 271,\n",
       " 'hindustan': 272,\n",
       " 'aeronautics': 273,\n",
       " 'limited': 274,\n",
       " 'advanced': 275,\n",
       " 'engines': 276,\n",
       " 'indigenous': 277,\n",
       " 'light': 278,\n",
       " 'combat': 279,\n",
       " 'aircraft': 280,\n",
       " 'means': 281,\n",
       " 'greater': 282,\n",
       " 'transfer': 283,\n",
       " 'engine': 284,\n",
       " 'ever': 285,\n",
       " 'before': 286,\n",
       " 'clear': 287,\n",
       " 'sign': 288,\n",
       " 'sell': 289,\n",
       " 'comfortable': 290,\n",
       " 'proceed': 291,\n",
       " '3bn': 292,\n",
       " 'battle': 293,\n",
       " 'tested': 294,\n",
       " 'mq': 295,\n",
       " '9b': 296,\n",
       " 'predator': 297,\n",
       " 'atomics': 298,\n",
       " 'set': 299,\n",
       " 'assembled': 300,\n",
       " 'fits': 301,\n",
       " \"'make\": 302,\n",
       " \"india'\": 303,\n",
       " 'campaign': 304,\n",
       " 'supplies': 305,\n",
       " '11': 306,\n",
       " '45': 307,\n",
       " 'supplier': 308,\n",
       " 'hopes': 309,\n",
       " 'primary': 310,\n",
       " 'provider': 311,\n",
       " 'coming': 312,\n",
       " 'immediate': 313,\n",
       " 'goal': 314,\n",
       " 'strengthen': 315,\n",
       " 'capacity': 316,\n",
       " 'counter': 317,\n",
       " 'base': 318,\n",
       " 'memory': 319,\n",
       " 'chip': 320,\n",
       " 'giant': 321,\n",
       " 'micron': 322,\n",
       " '825m': 323,\n",
       " 'build': 324,\n",
       " 'assembly': 325,\n",
       " 'creating': 326,\n",
       " 'thousands': 327,\n",
       " 'jobs': 328,\n",
       " 'equipment': 329,\n",
       " 'lam': 330,\n",
       " 'train': 331,\n",
       " '60': 332,\n",
       " '000': 333,\n",
       " 'engineers': 334,\n",
       " 'through': 335,\n",
       " 'network': 336,\n",
       " 'interconnected': 337,\n",
       " 'labs': 338,\n",
       " 'centres': 339,\n",
       " 'speed': 340,\n",
       " 'education': 341,\n",
       " 'workforce': 342,\n",
       " 'development': 343,\n",
       " 'applied': 344,\n",
       " 'materials': 345,\n",
       " 'machines': 346,\n",
       " 'producing': 347,\n",
       " 'semiconductors': 348,\n",
       " '400m': 349,\n",
       " 'establish': 350,\n",
       " 'engineering': 351,\n",
       " 'centre': 352,\n",
       " 'both': 353,\n",
       " 'talking': 354,\n",
       " 'cutting': 355,\n",
       " 'edge': 356,\n",
       " 'technologies': 357,\n",
       " 'seed': 358,\n",
       " 'shape': 359,\n",
       " 'lady': 360,\n",
       " 'jill': 361,\n",
       " '21': 362,\n",
       " 'mrs': 363,\n",
       " 'ups': 364,\n",
       " 'downs': 365,\n",
       " 'since': 366,\n",
       " 'seriously': 367,\n",
       " 'began': 368,\n",
       " 'courting': 369,\n",
       " 'bill': 370,\n",
       " 'clinton': 371,\n",
       " 'then': 372,\n",
       " 'george': 373,\n",
       " 'bush': 374,\n",
       " 'response': 375,\n",
       " 'measured': 376,\n",
       " 'overeager': 377,\n",
       " 'too': 378,\n",
       " 'forthcoming': 379,\n",
       " 'saw': 380,\n",
       " 'geopolitics': 381,\n",
       " 'its': 382,\n",
       " 'own': 383,\n",
       " 'order': 384,\n",
       " 'strategy': 385,\n",
       " 'nonalignment': 386,\n",
       " 'started': 387,\n",
       " 'jawaharlal': 388,\n",
       " 'nehru': 389,\n",
       " 'always': 390,\n",
       " 'deeply': 391,\n",
       " 'rooted': 392,\n",
       " 'camp': 393,\n",
       " 'other': 394,\n",
       " 'junior': 395,\n",
       " 'superpower': 396,\n",
       " 'left': 397,\n",
       " 'ideals': 398,\n",
       " 'describe': 399,\n",
       " 'altruism': 400,\n",
       " 'leading': 401,\n",
       " 'different': 402,\n",
       " 'kind': 403,\n",
       " 'considerably': 404,\n",
       " 'economic': 405,\n",
       " 'geopolitical': 406,\n",
       " 'heft': 407,\n",
       " 'formed': 408,\n",
       " 'bonds': 409,\n",
       " 'presidents': 410,\n",
       " 'barack': 411,\n",
       " 'donald': 412,\n",
       " 'sacrificed': 413,\n",
       " 'would': 414,\n",
       " 'go': 415,\n",
       " 'step': 416,\n",
       " 'probably': 417,\n",
       " 'take': 418,\n",
       " 'harder': 419,\n",
       " 'public': 420,\n",
       " 'stand': 421,\n",
       " 'seem': 422,\n",
       " 'disappointed': 423,\n",
       " 'repeated': 424,\n",
       " 'line': 425,\n",
       " 'era': 426,\n",
       " 'war': 427,\n",
       " 'mentioning': 428,\n",
       " 'speak': 429,\n",
       " 'beefing': 430,\n",
       " 'humanitarian': 431,\n",
       " 'assistance': 432,\n",
       " 'ukraine': 433,\n",
       " 'mention': 434,\n",
       " 'name': 435,\n",
       " 'either': 436,\n",
       " 'talk': 437,\n",
       " 'prosperous': 438,\n",
       " 'far': 439,\n",
       " 'pushed': 440,\n",
       " \"administration's\": 441,\n",
       " 'compromising': 442,\n",
       " 'ideal': 443,\n",
       " 'come': 444,\n",
       " 'making': 445,\n",
       " 'success': 446,\n",
       " 'iaf': 447,\n",
       " 'posing': 448,\n",
       " 'f': 449,\n",
       " '15': 450,\n",
       " 'eagle': 451,\n",
       " \"'exercise\": 452,\n",
       " 'cope': 453,\n",
       " \"2023'\": 454,\n",
       " 'station': 455,\n",
       " 'kalaikunda': 456,\n",
       " 'around': 457,\n",
       " '170': 458,\n",
       " 'km': 459,\n",
       " 'west': 460,\n",
       " 'kolkata': 461,\n",
       " '24th': 462,\n",
       " 'pose': 463,\n",
       " 'exercise': 464,\n",
       " 'militaries': 465,\n",
       " 'working': 466,\n",
       " 'closely': 467,\n",
       " 'together': 468,\n",
       " 'arrangements': 469,\n",
       " 'where': 470,\n",
       " 'use': 471,\n",
       " 'each': 472,\n",
       " \"other's\": 473,\n",
       " 'facilities': 474,\n",
       " 'refuelling': 475,\n",
       " 'maintenance': 476,\n",
       " 'purposes': 477,\n",
       " 'holding': 478,\n",
       " 'exercises': 479,\n",
       " \"they're\": 480,\n",
       " 'intelligence': 481,\n",
       " 'credit': 482,\n",
       " 'managing': 483,\n",
       " 'really': 484,\n",
       " 'limits': 485,\n",
       " 'sense': 486,\n",
       " 'getting': 487,\n",
       " 'you': 488,\n",
       " 'power': 489,\n",
       " 'signing': 490,\n",
       " 'fledged': 491,\n",
       " 'alliance': 492,\n",
       " 'differences': 493,\n",
       " 'recent': 494,\n",
       " 'particularly': 495,\n",
       " 'suffered': 496,\n",
       " 'expected': 497,\n",
       " 'announce': 498,\n",
       " 'anything': 499,\n",
       " 'understood': 500,\n",
       " 'continue': 501,\n",
       " 'overshadowing': 502,\n",
       " 'surprisingly': 503,\n",
       " 'announced': 504,\n",
       " 'six': 505,\n",
       " 'separate': 506,\n",
       " 'organization': 507,\n",
       " 'resolved': 508,\n",
       " 'including': 509,\n",
       " 'involved': 510,\n",
       " 'top': 511,\n",
       " 'trading': 512,\n",
       " '130bn': 513,\n",
       " 'goods': 514,\n",
       " 'delhi': 515,\n",
       " 'eighth': 516,\n",
       " 'largest': 517,\n",
       " 'while': 518,\n",
       " 'these': 519,\n",
       " 'numbers': 520,\n",
       " 'impressive': 521,\n",
       " 'analysts': 522,\n",
       " 'policymakers': 523,\n",
       " 'feel': 524,\n",
       " 'huge': 525,\n",
       " 'untapped': 526,\n",
       " 'burgeoning': 527,\n",
       " 'market': 528,\n",
       " 'expanding': 529,\n",
       " 'middle': 530,\n",
       " 'class': 531,\n",
       " 'positioning': 532,\n",
       " 'itself': 533,\n",
       " 'alternative': 534,\n",
       " 'manufacturing': 535,\n",
       " 'hub': 536,\n",
       " 'participate': 537,\n",
       " 'arrival': 538,\n",
       " 'ceremony': 539,\n",
       " 'south': 540,\n",
       " 'lawn': 541,\n",
       " 'thursday': 542,\n",
       " '22': 543,\n",
       " 'invite': 544,\n",
       " 'official': 545,\n",
       " '2': 546,\n",
       " '7': 547,\n",
       " 'million': 548,\n",
       " 'indians': 549,\n",
       " 'live': 550,\n",
       " 'firms': 551,\n",
       " 'interested': 552,\n",
       " 'proposal': 553,\n",
       " 'look': 554,\n",
       " 'supply': 555,\n",
       " 'chain': 556,\n",
       " 'dominance': 557,\n",
       " 'context': 558,\n",
       " 'resolution': 559,\n",
       " 'give': 560,\n",
       " 'impetus': 561,\n",
       " 'unlocking': 562,\n",
       " 'even': 563,\n",
       " 'sky': 564,\n",
       " 'limit': 565,\n",
       " 'critics': 566,\n",
       " 'questioned': 567,\n",
       " 'backsliding': 568,\n",
       " 'nationalist': 569,\n",
       " 'bharatiya': 570,\n",
       " 'janata': 571,\n",
       " 'bjp': 572,\n",
       " 'television': 573,\n",
       " 'interview': 574,\n",
       " 'week': 575,\n",
       " 'emphasised': 576,\n",
       " 'significance': 577,\n",
       " 'addressing': 578,\n",
       " 'protection': 579,\n",
       " 'muslim': 580,\n",
       " 'minority': 581,\n",
       " 'predominantly': 582,\n",
       " 'progressives': 583,\n",
       " 'disturbed': 584,\n",
       " 'happening': 585,\n",
       " 'realists': 586,\n",
       " 'centrists': 587,\n",
       " 'factor': 588,\n",
       " 'whole': 589,\n",
       " 'bipartisan': 590,\n",
       " 'agreement': 591,\n",
       " 'deeper': 592,\n",
       " 'broader': 593,\n",
       " 'certainly': 594,\n",
       " 'moved': 595,\n",
       " 'next': 596,\n",
       " 'level': 597,\n",
       " 'need': 598,\n",
       " 'benefit': 599}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokn = MyToken.word_index\n",
    "tokn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "066e44d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokn, open('tokn.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "589697ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Seq = []\n",
    "for line in file.split('\\n'):\n",
    "    token_list = MyToken.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        my_n_gram_sequence = token_list[:i+1]\n",
    "#       print(my_n_gram_sequence)\n",
    "        Input_Seq.append(my_n_gram_sequence)\n",
    "#         print(Input_Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db531810",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_length = max([len(Seq) for Seq in Input_Seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488f13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Max_length, open('Max_length.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d20684e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Input_Seq, open('Input_Seq.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faf3c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = np.array(pad_sequences(Input_Seq,maxlen=Max_length,padding=\"pre\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09fd8634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 99,  4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5936f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_seq[:,:-1]\n",
    "y = input_seq[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9d498a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y,num_classes=total_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5854252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Embedding(total_word,100,input_length=Max_length-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(keras.layers.Dense(total_word,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa08f260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 82, 100)           60000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 600)               90600     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 301200 (1.15 MB)\n",
      "Trainable params: 301200 (1.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4c45faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7a98a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "43/43 [==============================] - 14s 185ms/step - loss: 6.1868 - accuracy: 0.0481\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 8s 187ms/step - loss: 5.8061 - accuracy: 0.0576\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 8s 187ms/step - loss: 5.7249 - accuracy: 0.0576\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 8s 186ms/step - loss: 5.6672 - accuracy: 0.0576\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 8s 185ms/step - loss: 5.5868 - accuracy: 0.0576\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 8s 182ms/step - loss: 5.4792 - accuracy: 0.0591\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - 7s 172ms/step - loss: 5.3379 - accuracy: 0.0751\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - 8s 196ms/step - loss: 5.1708 - accuracy: 0.0883\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - 8s 195ms/step - loss: 4.9522 - accuracy: 0.1028\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - 8s 192ms/step - loss: 4.7098 - accuracy: 0.1211\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - 8s 187ms/step - loss: 4.4768 - accuracy: 0.1495\n",
      "Epoch 12/100\n",
      "43/43 [==============================] - 8s 180ms/step - loss: 4.2361 - accuracy: 0.1729\n",
      "Epoch 13/100\n",
      "43/43 [==============================] - 8s 194ms/step - loss: 3.9960 - accuracy: 0.1904\n",
      "Epoch 14/100\n",
      "43/43 [==============================] - 8s 191ms/step - loss: 3.7742 - accuracy: 0.2188\n",
      "Epoch 15/100\n",
      "43/43 [==============================] - 8s 196ms/step - loss: 3.5492 - accuracy: 0.2465\n",
      "Epoch 16/100\n",
      "43/43 [==============================] - 8s 186ms/step - loss: 3.3276 - accuracy: 0.2939\n",
      "Epoch 17/100\n",
      "43/43 [==============================] - 8s 192ms/step - loss: 3.1080 - accuracy: 0.3479\n",
      "Epoch 18/100\n",
      "43/43 [==============================] - 8s 186ms/step - loss: 2.9115 - accuracy: 0.3953\n",
      "Epoch 19/100\n",
      "43/43 [==============================] - 8s 181ms/step - loss: 2.7141 - accuracy: 0.4508\n",
      "Epoch 20/100\n",
      "43/43 [==============================] - 8s 186ms/step - loss: 2.5203 - accuracy: 0.5179\n",
      "Epoch 21/100\n",
      "43/43 [==============================] - 8s 195ms/step - loss: 2.3378 - accuracy: 0.5492\n",
      "Epoch 22/100\n",
      "43/43 [==============================] - 8s 194ms/step - loss: 2.1592 - accuracy: 0.6142\n",
      "Epoch 23/100\n",
      "43/43 [==============================] - 9s 203ms/step - loss: 1.9934 - accuracy: 0.6608\n",
      "Epoch 24/100\n",
      "43/43 [==============================] - 8s 194ms/step - loss: 1.8313 - accuracy: 0.7053\n",
      "Epoch 25/100\n",
      "43/43 [==============================] - 9s 198ms/step - loss: 1.6777 - accuracy: 0.7535\n",
      "Epoch 26/100\n",
      "43/43 [==============================] - 8s 180ms/step - loss: 1.5383 - accuracy: 0.7834\n",
      "Epoch 27/100\n",
      "43/43 [==============================] - 8s 191ms/step - loss: 1.4068 - accuracy: 0.8140\n",
      "Epoch 28/100\n",
      "43/43 [==============================] - 8s 193ms/step - loss: 1.2911 - accuracy: 0.8395\n",
      "Epoch 29/100\n",
      "43/43 [==============================] - 7s 168ms/step - loss: 1.1724 - accuracy: 0.8651\n",
      "Epoch 30/100\n",
      "43/43 [==============================] - 8s 187ms/step - loss: 1.0747 - accuracy: 0.8920\n",
      "Epoch 31/100\n",
      "43/43 [==============================] - 8s 194ms/step - loss: 0.9804 - accuracy: 0.9088\n",
      "Epoch 32/100\n",
      "43/43 [==============================] - 8s 183ms/step - loss: 0.8956 - accuracy: 0.9198\n",
      "Epoch 33/100\n",
      "43/43 [==============================] - 8s 181ms/step - loss: 0.8179 - accuracy: 0.9365\n",
      "Epoch 34/100\n",
      "43/43 [==============================] - 8s 183ms/step - loss: 0.7450 - accuracy: 0.9409\n",
      "Epoch 35/100\n",
      "43/43 [==============================] - 8s 187ms/step - loss: 0.6843 - accuracy: 0.9533\n",
      "Epoch 36/100\n",
      "43/43 [==============================] - 8s 185ms/step - loss: 0.6273 - accuracy: 0.9570\n",
      "Epoch 37/100\n",
      "43/43 [==============================] - 9s 212ms/step - loss: 0.5783 - accuracy: 0.9606\n",
      "Epoch 38/100\n",
      "43/43 [==============================] - 8s 182ms/step - loss: 0.5328 - accuracy: 0.9613\n",
      "Epoch 39/100\n",
      "43/43 [==============================] - 8s 182ms/step - loss: 0.4892 - accuracy: 0.9672\n",
      "Epoch 40/100\n",
      "43/43 [==============================] - 8s 184ms/step - loss: 0.4504 - accuracy: 0.9694\n",
      "Epoch 41/100\n",
      "43/43 [==============================] - 8s 182ms/step - loss: 0.4155 - accuracy: 0.9716\n",
      "Epoch 42/100\n",
      "43/43 [==============================] - 8s 181ms/step - loss: 0.3868 - accuracy: 0.9781\n",
      "Epoch 43/100\n",
      "43/43 [==============================] - 8s 182ms/step - loss: 0.3585 - accuracy: 0.9745\n",
      "Epoch 44/100\n",
      "43/43 [==============================] - 8s 176ms/step - loss: 0.3335 - accuracy: 0.9796\n",
      "Epoch 45/100\n",
      "43/43 [==============================] - 7s 166ms/step - loss: 0.3115 - accuracy: 0.9803\n",
      "Epoch 46/100\n",
      "43/43 [==============================] - 7s 172ms/step - loss: 0.2905 - accuracy: 0.9788\n",
      "Epoch 47/100\n",
      "43/43 [==============================] - 98s 2s/step - loss: 0.2727 - accuracy: 0.9818\n",
      "Epoch 48/100\n",
      "43/43 [==============================] - 4s 89ms/step - loss: 0.2558 - accuracy: 0.9825\n",
      "Epoch 49/100\n",
      "43/43 [==============================] - 8s 192ms/step - loss: 0.2402 - accuracy: 0.9818\n",
      "Epoch 50/100\n",
      "43/43 [==============================] - 8s 184ms/step - loss: 0.2273 - accuracy: 0.9832\n",
      "Epoch 51/100\n",
      "43/43 [==============================] - 8s 182ms/step - loss: 0.2135 - accuracy: 0.9847\n",
      "Epoch 52/100\n",
      "43/43 [==============================] - 8s 182ms/step - loss: 0.2024 - accuracy: 0.9840\n",
      "Epoch 53/100\n",
      "43/43 [==============================] - 8s 179ms/step - loss: 0.1905 - accuracy: 0.9854\n",
      "Epoch 54/100\n",
      "43/43 [==============================] - 8s 180ms/step - loss: 0.1817 - accuracy: 0.9847\n",
      "Epoch 55/100\n",
      "43/43 [==============================] - 8s 180ms/step - loss: 0.1732 - accuracy: 0.9861\n",
      "Epoch 56/100\n",
      "43/43 [==============================] - 8s 179ms/step - loss: 0.1639 - accuracy: 0.9847\n",
      "Epoch 57/100\n",
      "43/43 [==============================] - 8s 180ms/step - loss: 0.1561 - accuracy: 0.9832\n",
      "Epoch 58/100\n",
      "43/43 [==============================] - 8s 179ms/step - loss: 0.1493 - accuracy: 0.9840\n",
      "Epoch 59/100\n",
      "43/43 [==============================] - 8s 180ms/step - loss: 0.1428 - accuracy: 0.9861\n",
      "Epoch 60/100\n",
      "43/43 [==============================] - 8s 180ms/step - loss: 0.1357 - accuracy: 0.9854\n",
      "Epoch 61/100\n",
      "43/43 [==============================] - 9s 211ms/step - loss: 0.1310 - accuracy: 0.9854\n",
      "Epoch 62/100\n",
      "43/43 [==============================] - 10s 230ms/step - loss: 0.1252 - accuracy: 0.9861\n",
      "Epoch 63/100\n",
      "43/43 [==============================] - 9s 209ms/step - loss: 0.1200 - accuracy: 0.9854\n",
      "Epoch 64/100\n",
      "43/43 [==============================] - 9s 200ms/step - loss: 0.1164 - accuracy: 0.9832\n",
      "Epoch 65/100\n",
      "43/43 [==============================] - 9s 213ms/step - loss: 0.1118 - accuracy: 0.9869\n",
      "Epoch 66/100\n",
      "43/43 [==============================] - 9s 199ms/step - loss: 0.1076 - accuracy: 0.9861\n",
      "Epoch 67/100\n",
      "43/43 [==============================] - 8s 191ms/step - loss: 0.1040 - accuracy: 0.9854\n",
      "Epoch 68/100\n",
      "43/43 [==============================] - 8s 194ms/step - loss: 0.1009 - accuracy: 0.9854\n",
      "Epoch 69/100\n",
      "43/43 [==============================] - 8s 195ms/step - loss: 0.0971 - accuracy: 0.9861\n",
      "Epoch 70/100\n",
      "43/43 [==============================] - 8s 194ms/step - loss: 0.0941 - accuracy: 0.9861\n",
      "Epoch 71/100\n",
      "43/43 [==============================] - 8s 191ms/step - loss: 0.0916 - accuracy: 0.9869\n",
      "Epoch 72/100\n",
      "43/43 [==============================] - 8s 192ms/step - loss: 0.0882 - accuracy: 0.9869\n",
      "Epoch 73/100\n",
      "43/43 [==============================] - 8s 193ms/step - loss: 0.0858 - accuracy: 0.9861\n",
      "Epoch 74/100\n",
      "43/43 [==============================] - 8s 192ms/step - loss: 0.0836 - accuracy: 0.9847\n",
      "Epoch 75/100\n",
      "43/43 [==============================] - 8s 196ms/step - loss: 0.0821 - accuracy: 0.9847\n",
      "Epoch 76/100\n",
      "43/43 [==============================] - 8s 187ms/step - loss: 0.0795 - accuracy: 0.9861\n",
      "Epoch 77/100\n",
      "43/43 [==============================] - 8s 193ms/step - loss: 0.0763 - accuracy: 0.9854\n",
      "Epoch 78/100\n",
      "43/43 [==============================] - 8s 195ms/step - loss: 0.0743 - accuracy: 0.9861\n",
      "Epoch 79/100\n",
      "43/43 [==============================] - 8s 195ms/step - loss: 0.0731 - accuracy: 0.9854\n",
      "Epoch 80/100\n",
      "43/43 [==============================] - 8s 197ms/step - loss: 0.0734 - accuracy: 0.9854\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 8s 197ms/step - loss: 0.0696 - accuracy: 0.9861\n",
      "Epoch 82/100\n",
      "43/43 [==============================] - 8s 192ms/step - loss: 0.0681 - accuracy: 0.9869\n",
      "Epoch 83/100\n",
      "43/43 [==============================] - 8s 190ms/step - loss: 0.0669 - accuracy: 0.9840\n",
      "Epoch 84/100\n",
      "43/43 [==============================] - 8s 194ms/step - loss: 0.0655 - accuracy: 0.9876\n",
      "Epoch 85/100\n",
      "43/43 [==============================] - 8s 197ms/step - loss: 0.0645 - accuracy: 0.9861\n",
      "Epoch 86/100\n",
      "43/43 [==============================] - 9s 200ms/step - loss: 0.0629 - accuracy: 0.9883\n",
      "Epoch 87/100\n",
      "43/43 [==============================] - 7s 161ms/step - loss: 0.0608 - accuracy: 0.9861\n",
      "Epoch 88/100\n",
      "43/43 [==============================] - 9s 199ms/step - loss: 0.0609 - accuracy: 0.9854\n",
      "Epoch 89/100\n",
      "43/43 [==============================] - 8s 191ms/step - loss: 0.0592 - accuracy: 0.9876\n",
      "Epoch 90/100\n",
      "43/43 [==============================] - 12s 283ms/step - loss: 0.0576 - accuracy: 0.9847\n",
      "Epoch 91/100\n",
      "43/43 [==============================] - 11s 252ms/step - loss: 0.0577 - accuracy: 0.9840\n",
      "Epoch 92/100\n",
      "43/43 [==============================] - 10s 224ms/step - loss: 0.0554 - accuracy: 0.9876\n",
      "Epoch 93/100\n",
      "43/43 [==============================] - 11s 266ms/step - loss: 0.0550 - accuracy: 0.9854\n",
      "Epoch 94/100\n",
      "43/43 [==============================] - 10s 236ms/step - loss: 0.0539 - accuracy: 0.9869\n",
      "Epoch 95/100\n",
      "43/43 [==============================] - 10s 226ms/step - loss: 0.0542 - accuracy: 0.9854\n",
      "Epoch 96/100\n",
      "43/43 [==============================] - 10s 229ms/step - loss: 0.0525 - accuracy: 0.9869\n",
      "Epoch 97/100\n",
      "43/43 [==============================] - 9s 205ms/step - loss: 0.0517 - accuracy: 0.9861\n",
      "Epoch 98/100\n",
      "43/43 [==============================] - 8s 188ms/step - loss: 0.0511 - accuracy: 0.9861\n",
      "Epoch 99/100\n",
      "43/43 [==============================] - 8s 179ms/step - loss: 0.0500 - accuracy: 0.9854\n",
      "Epoch 100/100\n",
      "43/43 [==============================] - 8s 176ms/step - loss: 0.0508 - accuracy: 0.9854\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X,y,epochs=100,verbose=1)\n",
    "model.save(\"my_model.keras\", hist)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ae81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
